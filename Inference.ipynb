{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6193b2",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "A module for interacting with integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb1a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MODEL_DIR, load_model\n",
    "from data_loader import ReviewsDataset, EssaysDataset\n",
    "from captum.attr import IntegratedGradients, LimeBase\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def show_IG(model, dataset, ex_id=None, filter_params=None):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    if not filter_params:  filter_params = {}\n",
    "    if not ex_id:  ex_id = dataset.filter_load(**filter_params, format='index')[0]\n",
    "    ig = IntegratedGradients(model.forward_emb)\n",
    "    example = dataset[ex_id][0].unsqueeze(dim=0).to(device)\n",
    "    pred = model(example).argmax().cpu()\n",
    "    \n",
    "    # NOTE:  'baselines' should be zero tensor by default, which corresponds to having all <PAD> tokens.\n",
    "    attributions = ig.attribute(inputs=model.get_embeddings(example), baselines=None, target=pred)\n",
    "    scores = np.mean(attributions.detach().cpu().numpy(), axis=2).squeeze()\n",
    "\n",
    "    encoding, label = dataset.__getitem__(ex_id, format='encoding')\n",
    "\n",
    "    # create color mapping\n",
    "    color_mapping = [   # RED => GREEN\n",
    "        ((206, 35, 35), scores.min()),\n",
    "        ((255, 255, 255), np.median(scores)),\n",
    "        ((22, 206, 16), scores.max())\n",
    "    ]\n",
    "    \n",
    "    # CREDIT:  https://databasecamp.de/en/ml/integrated-gradients-nlp  (reference for creating HTML display)\n",
    "    def create_color_map(color_coords, color_bounds):\n",
    "        def to_cmap_coord(x, level=0.0):  return( (level, np.interp(x, xp=[0,255], fp=[0,1]), np.interp(x, xp=[0,255], fp=[0,1])) )\n",
    "\n",
    "        cmap_price_bounds = [np.interp(p, xp=[min(color_bounds), max(color_bounds)], fp=[0, 1]) for p in color_bounds]\n",
    "\n",
    "        c_dict = {\n",
    "            'red':tuple(to_cmap_coord(color_coords[i][0], cmap_price_bounds[i]) for i in range(len(color_coords))),\n",
    "            'green':tuple(to_cmap_coord(color_coords[i][1], cmap_price_bounds[i]) for i in range(len(color_coords))),\n",
    "            'blue':tuple(to_cmap_coord(color_coords[i][2], cmap_price_bounds[i]) for i in range(len(color_coords))),\n",
    "        }\n",
    "        \n",
    "        return (matplotlib.colors.LinearSegmentedColormap('cmap', segmentdata=c_dict))\n",
    "    c_map = create_color_map([c[0] for c in color_mapping], [c[1] for c in color_mapping])\n",
    "    norm = matplotlib.colors.Normalize(vmin=scores.min(), vmax=scores.max())\n",
    "\n",
    "    def build_html(text, c_map, norm, encoding, scores):\n",
    "        def highlight(token, score):\n",
    "            return f\"<mark style=\\\"margin: 0; padding: 0; background-color:{matplotlib.colors.rgb2hex(c_map(norm(score)))}\\\">{token}</mark>\"\n",
    "        prev = (0, 0)\n",
    "        cur_html = \"\"\n",
    "        for i in range(len(encoding)):\n",
    "            cur_html = cur_html + text[prev[1]: encoding.offsets[i][0]]\n",
    "            cur_html = cur_html + highlight(encoding.tokens[i], scores[i])\n",
    "            prev = encoding.offsets[i]\n",
    "        return HTML(cur_html)\n",
    "    \n",
    "    #if dataset.score_type == 'categorical':  pred = pred + 1\n",
    "    pred = pred.item()\n",
    "    print(\"Example id:  \", ex_id)\n",
    "    if 'category' in filter_params:\n",
    "        print(\"Category:  \", filter_params['category'])\n",
    "    print(f\"{'Predicted Rating:' : <18}\", pred if dataset.score_type == 'binary' else pred + 1)\n",
    "    print(f\"{'Actual Rating:' : <18}\", label)\n",
    "\n",
    "    return build_html(dataset.__getitem__(ex_id, format='raw')[0], c_map, norm, encoding, scores)\n",
    "\n",
    "def show_LIME(model, dataset, num_samples=50, show_text=True, show_probs=True, ex_id=None, filter_params=None):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    model.eval()\n",
    "    if not filter_params:  filter_params = {}\n",
    "    if not ex_id:  ex_id = dataset.filter_load(**filter_params, format='index')[0]\n",
    "    pred = model(dataset[ex_id][0].unsqueeze(dim=0).to(device)).argmax().cpu().item()\n",
    "\n",
    "    lime = LimeTextExplainer(class_names=list(sorted(dataset.get_unique_labels())))\n",
    "    def model_prob_wrapper(dataset, model):\n",
    "        def forward(text):\n",
    "            print(\"TEXT: \", len(text))\n",
    "            return nn.functional.softmax( model.forward(dataset.encode(text, mode='train').to(device)), dim=1 ).detach().cpu().numpy()\n",
    "        return forward\n",
    "    \n",
    "    text, label = dataset.__getitem__(ex_id, format='raw')\n",
    "    print(\"Example id:  \", ex_id)\n",
    "    if 'category' in filter_params:\n",
    "        print(\"Category:  \", filter_params['category'])\n",
    "    print(f\"{'Predicted Rating:' : <18}\", pred if dataset.score_type == 'binary' else pred + 1)\n",
    "    print(f\"{'Actual Rating:' : <18}\", label)\n",
    "    explanation = lime.explain_instance(text, model_prob_wrapper(dataset, model), labels=(pred,), num_samples=num_samples)\n",
    "    \n",
    "    return explanation.show_in_notebook(text=show_text, labels=(pred,), predict_proba=show_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9a7ffb",
   "metadata": {},
   "source": [
    "### IG Experiments\n",
    "\n",
    "* Not applicable to regression models (standardized scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f20cf911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example id:   36148\n",
      "Predicted Rating:  5\n",
      "Actual Rating:     5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"margin: 0; padding: 0; background-color:#fcf0f0\">the</mark> <mark style=\"margin: 0; padding: 0; background-color:#f0fcef\">bo</mark><mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">x</mark> <mark style=\"margin: 0; padding: 0; background-color:#9dea9b\">sto</mark><mark style=\"margin: 0; padding: 0; background-color:#fbfefb\">res</mark> <mark style=\"margin: 0; padding: 0; background-color:#f7fdf7\">want</mark> <mark style=\"margin: 0; padding: 0; background-color:#ecfbeb\">about</mark> <mark style=\"margin: 0; padding: 0; background-color:#f7dada\">$</mark><mark style=\"margin: 0; padding: 0; background-color:#fbefef\">5</mark><mark style=\"margin: 0; padding: 0; background-color:#fbfefb\">-</mark><mark style=\"margin: 0; padding: 0; background-color:#fef8f8\">7</mark> <mark style=\"margin: 0; padding: 0; background-color:#f9fef9\">per</mark> <mark style=\"margin: 0; padding: 0; background-color:#fefafa\">b</mark><mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">ul</mark><mark style=\"margin: 0; padding: 0; background-color:#fefafa\">b</mark> <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">for</mark> <mark style=\"margin: 0; padding: 0; background-color:#ecfbeb\">these</mark> <mark style=\"margin: 0; padding: 0; background-color:#df7171\">thing</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">s</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">.</mark>  <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">i</mark> <mark style=\"margin: 0; padding: 0; background-color:#ffffff\">got</mark> <mark style=\"margin: 0; padding: 0; background-color:#ecfbeb\">these</mark> <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">for</mark> <mark style=\"margin: 0; padding: 0; background-color:#f7dada\">$</mark><mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">1</mark> <mark style=\"margin: 0; padding: 0; background-color:#f9fef9\">per</mark> <mark style=\"margin: 0; padding: 0; background-color:#fefafa\">b</mark><mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">ul</mark><mark style=\"margin: 0; padding: 0; background-color:#fefafa\">b</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">.</mark>  <mark style=\"margin: 0; padding: 0; background-color:#ffffff\">they</mark> <mark style=\"margin: 0; padding: 0; background-color:#f4cece\">work</mark> <mark style=\"margin: 0; padding: 0; background-color:#16ce10\">great</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">.</mark>  <mark style=\"margin: 0; padding: 0; background-color:#f6d5d5\">some</mark><mark style=\"margin: 0; padding: 0; background-color:#fbfefb\">one</mark> <mark style=\"margin: 0; padding: 0; background-color:#f2fcf1\">sa</mark><mark style=\"margin: 0; padding: 0; background-color:#f5d2d2\">id</mark> <mark style=\"margin: 0; padding: 0; background-color:#ffffff\">they</mark> <mark style=\"margin: 0; padding: 0; background-color:#f2c6c6\">are</mark> <mark style=\"margin: 0; padding: 0; background-color:#ce2323\">not</mark> <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">as</mark> <mark style=\"margin: 0; padding: 0; background-color:#fefafa\">b</mark><mark style=\"margin: 0; padding: 0; background-color:#daf7d9\">right</mark> <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">as</mark> <mark style=\"margin: 0; padding: 0; background-color:#fcf0f0\">the</mark> <mark style=\"margin: 0; padding: 0; background-color:#fef8f8\">on</mark><mark style=\"margin: 0; padding: 0; background-color:#f0fcef\">es</mark> <mark style=\"margin: 0; padding: 0; background-color:#fefcfc\">that</mark> <mark style=\"margin: 0; padding: 0; background-color:#fcf3f3\">came</mark> <mark style=\"margin: 0; padding: 0; background-color:#e2f9e1\">with</mark> <mark style=\"margin: 0; padding: 0; background-color:#fcf0f0\">the</mark> <mark style=\"margin: 0; padding: 0; background-color:#f9fef9\">light</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">s</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">.</mark>  <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">i</mark> <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">f</mark><mark style=\"margin: 0; padding: 0; background-color:#f9e5e5\">ound</mark> <mark style=\"margin: 0; padding: 0; background-color:#fefcfc\">that</mark> <mark style=\"margin: 0; padding: 0; background-color:#ffffff\">they</mark> <mark style=\"margin: 0; padding: 0; background-color:#f2c6c6\">are</mark> <mark style=\"margin: 0; padding: 0; background-color:#f2fcf1\">act</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">ually</mark> <mark style=\"margin: 0; padding: 0; background-color:#ffffff\">a</mark> <mark style=\"margin: 0; padding: 0; background-color:#fefafa\">b</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">it</mark> <mark style=\"margin: 0; padding: 0; background-color:#fefafa\">b</mark><mark style=\"margin: 0; padding: 0; background-color:#daf7d9\">right</mark><mark style=\"margin: 0; padding: 0; background-color:#d8f7d7\">er</mark> <mark style=\"margin: 0; padding: 0; background-color:#f3fdf3\">so</mark> <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">i</mark> <mark style=\"margin: 0; padding: 0; background-color:#ffffff\">am</mark> <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">very</mark> <mark style=\"margin: 0; padding: 0; background-color:#e0f8df\">ha</mark><mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">pp</mark><mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">y</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">.</mark>  <mark style=\"margin: 0; padding: 0; background-color:#fefafa\">b</mark><mark style=\"margin: 0; padding: 0; background-color:#fefcfc\">t</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">w</mark> <mark style=\"margin: 0; padding: 0; background-color:#fef8f8\">be</mark><mark style=\"margin: 0; padding: 0; background-color:#eafbe9\">co</mark><mark style=\"margin: 0; padding: 0; background-color:#f0fcef\">me</mark> <mark style=\"margin: 0; padding: 0; background-color:#ffffff\">a</mark> <mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">p</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">ri</mark><mark style=\"margin: 0; padding: 0; background-color:#f0fcef\">me</mark> <mark style=\"margin: 0; padding: 0; background-color:#f0fcef\">me</mark><mark style=\"margin: 0; padding: 0; background-color:#f5d0d0\">m</mark><mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">ber</mark> <mark style=\"margin: 0; padding: 0; background-color:#dcf8db\">and</mark> <mark style=\"margin: 0; padding: 0; background-color:#eafbe9\">you</mark> <mark style=\"margin: 0; padding: 0; background-color:#fcf0f0\">get</mark> <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">f</mark><mark style=\"margin: 0; padding: 0; background-color:#fef8f8\">re</mark><mark style=\"margin: 0; padding: 0; background-color:#f5fdf5\">e</mark> <mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">sh</mark><mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">i</mark><mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">pp</mark><mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">ing</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">.</mark>  <mark style=\"margin: 0; padding: 0; background-color:#fdfffd\">i</mark> <mark style=\"margin: 0; padding: 0; background-color:#fef8f8\">re</mark><mark style=\"margin: 0; padding: 0; background-color:#fefcfc\">c</mark><mark style=\"margin: 0; padding: 0; background-color:#f2fcf1\">ent</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">ly</mark> <mark style=\"margin: 0; padding: 0; background-color:#f5fdf5\">jo</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">in</mark><mark style=\"margin: 0; padding: 0; background-color:#fdf5f5\">ed</mark> <mark style=\"margin: 0; padding: 0; background-color:#dcf8db\">and</mark> <mark style=\"margin: 0; padding: 0; background-color:#f2fcf1\">have</mark> <mark style=\"margin: 0; padding: 0; background-color:#e8fae7\">al</mark><mark style=\"margin: 0; padding: 0; background-color:#f2c3c3\">read</mark><mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">y</mark> <mark style=\"margin: 0; padding: 0; background-color:#fef8f8\">re</mark><mark style=\"margin: 0; padding: 0; background-color:#fefcfc\">c</mark><mark style=\"margin: 0; padding: 0; background-color:#fbfefb\">ou</mark><mark style=\"margin: 0; padding: 0; background-color:#fffdfd\">p</mark><mark style=\"margin: 0; padding: 0; background-color:#fdf5f5\">ed</mark> <mark style=\"margin: 0; padding: 0; background-color:#caf4c9\">my</mark> <mark style=\"margin: 0; padding: 0; background-color:#ffffff\">in</mark><mark style=\"margin: 0; padding: 0; background-color:#bbf1b9\">v</mark><mark style=\"margin: 0; padding: 0; background-color:#8ee78b\">est</mark><mark style=\"margin: 0; padding: 0; background-color:#fcf2f2\">ment</mark><mark style=\"margin: 0; padding: 0; background-color:#ffffff\">.</mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SHORT_LEN = 1000\n",
    "LARGE_LEN = [1000, 6000]\n",
    "#  ------------  REVIEWS EXPERIMENTS  ------------  #\n",
    "\n",
    "# Categorical, Short sequences, vocab_size ~ {500, 1000, 5000}\n",
    "show_IG(load_model('reviews_cat_tk500_sq500'), ReviewsDataset(score_type='categorical', tokenizer='reviews_tokenizer_500', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_IG(load_model('reviews_cat_tk1000_sq1000'), ReviewsDataset(score_type='categorical', tokenizer='reviews_tokenizer', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_IG(load_model('reviews_cat_tk5000_sq1000'), ReviewsDataset(score_type='categorical', tokenizer='reviews_tokenizer_5000', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "\n",
    "# Binary, Short sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_IG(load_model('reviews_bin_tk500_sq500'), ReviewsDataset(score_type='binary', tokenizer='reviews_tokenizer_500', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_IG(load_model('reviews_bin_tk1000_sq1000'), ReviewsDataset(score_type='binary', tokenizer='reviews_tokenizer', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_IG(load_model('reviews_bin_tk5000_sq500'), ReviewsDataset(score_type='binary', tokenizer='reviews_tokenizer_5000', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "\n",
    "# Categorical, Longer sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_IG(load_model('reviews_cat_tk500_sq500'), ReviewsDataset(score_type='categorical', tokenizer='reviews_tokenizer_500', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_IG(load_model('reviews_cat_tk1000_sq1000'), ReviewsDataset(score_type='categorical', tokenizer='reviews_tokenizer', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_IG(load_model('reviews_cat_tk5000_sq1000'), ReviewsDataset(score_type='categorical', tokenizer='reviews_tokenizer_5000', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "\n",
    "# Binary, Longer sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_IG(load_model('reviews_bin_tk500_sq500'), ReviewsDataset(score_type='binary', tokenizer='reviews_tokenizer_500', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_IG(load_model('reviews_bin_tk1000_sq1000'), ReviewsDataset(score_type='binary', tokenizer='reviews_tokenizer', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_IG(load_model('reviews_bin_tk5000_sq500'), ReviewsDataset(score_type='binary', tokenizer='reviews_tokenizer_5000', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "\n",
    "\n",
    "#  ------------  ESSAYS EXPERIMENTS  ------------  #\n",
    "\n",
    "# Categorical, Short sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_IG(load_model('essays_cat_tk500_sq1000'), EssaysDataset(score_type='categorical', tokenizer='essays_tokenizer_500', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_IG(load_model('essays_cat_tk1000_sq500'), EssaysDataset(score_type='categorical', tokenizer='essays_tokenizer', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_IG(load_model('essays_cat_tk5000_sq500'), EssaysDataset(score_type='categorical', tokenizer='essays_tokenizer_5000', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "\n",
    "# Binary, Short sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_IG(load_model('essays_bin_tk500_sq1000'), EssaysDataset(score_type='binary', tokenizer='essays_tokenizer_500', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_IG(load_model('essays_bin_tk1000_sq1000'), EssaysDataset(score_type='binary', tokenizer='essays_tokenizer', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_IG(load_model('essays_bin_tk5000_sq1000'), EssaysDataset(score_type='binary', tokenizer='essays_tokenizer_5000', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "\n",
    "# Categorical, Longer sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_IG(load_model('essays_cat_tk500_sq1000'), EssaysDataset(score_type='categorical', tokenizer='essays_tokenizer_500', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_IG(load_model('essays_cat_tk1000_sq500'), EssaysDataset(score_type='categorical', tokenizer='essays_tokenizer', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_IG(load_model('essays_cat_tk5000_sq500'), EssaysDataset(score_type='categorical', tokenizer='essays_tokenizer_5000', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "\n",
    "# Binary, Longer sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_IG(load_model('essays_bin_tk500_sq1000'), EssaysDataset(score_type='binary', tokenizer='essays_tokenizer_500', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_IG(load_model('essays_bin_tk1000_sq1000'), EssaysDataset(score_type='binary', tokenizer='essays_tokenizer', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_IG(load_model('essays_bin_tk5000_sq1000'), EssaysDataset(score_type='binary', tokenizer='essays_tokenizer_5000', load_mode='lazy'), filter_params={'length': LARGE_LEN})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6caf89b",
   "metadata": {},
   "source": [
    "### LIME Experiments\n",
    "* NOTE:  Should be applicable to regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_LEN = 1000\n",
    "LARGE_LEN = [1000, 6000]\n",
    "#  ------------  REVIEWS EXPERIMENTS  ------------  #\n",
    "\n",
    "# Categorical, Short sequences, vocab_size ~ {500, 1000, 5000}\n",
    "show_LIME(load_model('reviews_cat_tk500_sq500'), ReviewsDataset(score_type='categorical', seq_len=500, tokenizer='reviews_tokenizer_500', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_LIME(load_model('reviews_cat_tk1000_sq1000'), ReviewsDataset(score_type='categorical', seq_len=1000, tokenizer='reviews_tokenizer', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_LIME(load_model('reviews_cat_tk5000_sq1000'), ReviewsDataset(score_type='categorical', seq_len=1000, tokenizer='reviews_tokenizer_5000', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "\n",
    "# Binary, Short sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_LIME(load_model('reviews_bin_tk500_sq500'), ReviewsDataset(score_type='binary', seq_len=500, tokenizer='reviews_tokenizer_500', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_LIME(load_model('reviews_bin_tk1000_sq1000'), ReviewsDataset(score_type='binary', seq_len=1000, tokenizer='reviews_tokenizer', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_LIME(load_model('reviews_bin_tk5000_sq500'), ReviewsDataset(score_type='binary', seq_len=500, tokenizer='reviews_tokenizer_5000', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "\n",
    "# Categorical, Longer sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_LIME(load_model('reviews_cat_tk500_sq500'), ReviewsDataset(score_type='categorical', seq_len=500, tokenizer='reviews_tokenizer_500', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_LIME(load_model('reviews_cat_tk1000_sq1000'), ReviewsDataset(score_type='categorical', seq_len=1000, tokenizer='reviews_tokenizer', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_LIME(load_model('reviews_cat_tk5000_sq1000'), ReviewsDataset(score_type='categorical', seq_len=1000, tokenizer='reviews_tokenizer_5000', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "\n",
    "# Binary, Longer sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_LIME(load_model('reviews_bin_tk500_sq500'), ReviewsDataset(score_type='binary', seq_len=500, tokenizer='reviews_tokenizer_500', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_LIME(load_model('reviews_bin_tk1000_sq1000'), ReviewsDataset(score_type='binary', seq_len=1000, tokenizer='reviews_tokenizer', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_LIME(load_model('reviews_bin_tk5000_sq500'), ReviewsDataset(score_type='binary', seq_len=500, tokenizer='reviews_tokenizer_5000', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "\n",
    "\n",
    "#  ------------  ESSAYS EXPERIMENTS  ------------  #\n",
    "\n",
    "# Categorical, Short sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_LIME(load_model('essays_cat_tk500_sq1000'), EssaysDataset(score_type='categorical', seq_len=1000, tokenizer='essays_tokenizer_500', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_LIME(load_model('essays_cat_tk1000_sq500'), EssaysDataset(score_type='categorical', seq_len=500, tokenizer='essays_tokenizer', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_LIME(load_model('essays_cat_tk5000_sq500'), EssaysDataset(score_type='categorical', seq_len=500, tokenizer='essays_tokenizer_5000', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "\n",
    "# Binary, Short sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_LIME(load_model('essays_bin_tk500_sq500'), EssaysDataset(score_type='binary', seq_len=500, tokenizer='essays_tokenizer_500', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_LIME(load_model('essays_bin_tk1000_sq500'), EssaysDataset(score_type='binary', seq_len=500, tokenizer='essays_tokenizer', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "# show_LIME(load_model('essays_bin_tk5000_sq500'), EssaysDataset(score_type='binary', seq_len=500, tokenizer='essays_tokenizer_5000', load_mode='lazy'), filter_params={'length': SHORT_LEN})\n",
    "\n",
    "# Categorical, Longer sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_LIME(load_model('essays_cat_tk500_sq1000'), EssaysDataset(score_type='categorical', seq_len=1000, tokenizer='essays_tokenizer_500', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_LIME(load_model('essays_cat_tk1000_sq500'), EssaysDataset(score_type='categorical', seq_len=500, tokenizer='essays_tokenizer', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_LIME(load_model('essays_cat_tk5000_sq500'), EssaysDataset(score_type='categorical', seq_len=500, tokenizer='essays_tokenizer_5000', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "\n",
    "# Binary, Longer sequences, vocab_size ~ {500, 1000, 5000}\n",
    "# show_LIME(load_model('essays_bin_tk500_sq500'), EssaysDataset(score_type='binary', seq_len=500, tokenizer='essays_tokenizer_500', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_LIME(load_model('essays_bin_tk1000_sq500'), EssaysDataset(score_type='binary', seq_len=500, tokenizer='essays_tokenizer', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n",
    "# show_LIME(load_model('essays_bin_tk5000_sq500'), EssaysDataset(score_type='binary', seq_len=500, tokenizer='essays_tokenizer_5000', load_mode='lazy'), filter_params={'length': LARGE_LEN})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c124da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_LIME(load_model('essays_trans_cat'), EssaysDataset(score_type='categorical', seq_len=500, load_mode='lazy'),\n",
    "#           num_samples=500, ex_id=290)\n",
    "# show_LIME(load_model('reviews_trans_cat'), ReviewsDataset(score_type='categorical', seq_len=500, load_mode='lazy'),\n",
    "#           num_samples=500, ex_id=1301)\n",
    "# show_LIME(load_model('essays_dan_bin'), ReviewsDataset(score_type='binary', seq_len=500, load_mode='lazy'),\n",
    "#           num_samples=500, filter_params={'category': 'appliances', 'length': 800, 'score': 0})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "742a6ce35c6ceb91169cd31c6af0c4bf02ab04253b75a8d888eba7d2f850f99d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
